{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T15:23:02.095719Z",
     "start_time": "2025-10-02T15:22:36.775030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparameter tuning basique pour RandomForest et XGBoost\n",
    "# - get_dummies (encodage simple)\n",
    "# - split train/test\n",
    "# - évalue défaut vs après tuning (CV=5)\n",
    "# - imprime RMSE/MAE/R² avant/après\n",
    "# - XGBoost est optionnel (skip si non installé)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# XGBoost si dispo\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    HAS_XGB = True\n",
    "except Exception:\n",
    "    HAS_XGB = False\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "    return rmse, mae, r2\n",
    "\n",
    "def print_scores(title, y_true, y_pred):\n",
    "    rmse, mae, r2 = metrics(y_true, y_pred)\n",
    "    print(f\"{title:30s} -> RMSE:{rmse:8.2f}  MAE:{mae:8.2f}  R²:{r2:7.4f}\")\n",
    "    return rmse, mae, r2\n",
    "\n",
    "# 1) Charger les données\n",
    "csv_path = \"ensurance.csv\"  # change si nécessaire\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# 2) X (features) et y (target)\n",
    "y = df[\"charges\"]\n",
    "X = df.drop(columns=[\"charges\"])\n",
    "\n",
    "# 3) Encodage simple des catégorielles\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# 4) Split 80/20\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"=== Scores par défaut (avant tuning) ===\")\n",
    "default_results = []\n",
    "\n",
    "# Baseline (prédit la moyenne)\n",
    "baseline = y_train.mean()\n",
    "y_pred_base = np.full_like(y_test, baseline, dtype=float)\n",
    "default_results.append((\"Baseline-Mean\", *print_scores(\"Baseline-Mean\", y_test, y_pred_base)))\n",
    "\n",
    "# A) RandomForest - défaut\n",
    "rf_default = RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1)\n",
    "rf_default.fit(X_train, y_train)\n",
    "y_pred_rf_def = rf_default.predict(X_test)\n",
    "default_results.append((\"RandomForest (default)\", *print_scores(\"RandomForest (default)\", y_test, y_pred_rf_def)))\n",
    "\n",
    "# B) XGBoost - défaut (si dispo)\n",
    "if HAS_XGB:\n",
    "    xgb_default = XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_estimators=300,\n",
    "        tree_method=\"auto\"\n",
    "    )\n",
    "    xgb_default.fit(X_train, y_train)\n",
    "    y_pred_xgb_def = xgb_default.predict(X_test)\n",
    "    default_results.append((\"XGBoost (default)\", *print_scores(\"XGBoost (default)\", y_test, y_pred_xgb_def)))\n",
    "else:\n",
    "    print(\"XGBoost non installé -> on saute cette partie (pip install xgboost)\")\n",
    "\n",
    "print(\"\\n=== Tuning (RandomizedSearchCV, CV=5) ===\")\n",
    "tuned_results = []\n",
    "\n",
    "# Paramètres RF (simples)\n",
    "param_dist_rf = {\n",
    "    \"n_estimators\": [200, 400, 800, 1200],\n",
    "    \"max_depth\": [None, 5, 10, 20, 30],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1)\n",
    "rf_search = RandomizedSearchCV(\n",
    "    rf,\n",
    "    param_distributions=param_dist_rf,\n",
    "    n_iter=20,\n",
    "    scoring=\"neg_mean_squared_error\",  # on optimise la MSE (-> RMSE)\n",
    "    cv=5,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    refit=True,  # refit sur tout X_train avec les meilleurs hyperparamètres\n",
    ")\n",
    "rf_search.fit(X_train, y_train)\n",
    "\n",
    "rf_best = rf_search.best_estimator_\n",
    "y_pred_rf_best = rf_best.predict(X_test)\n",
    "rmse, mae, r2 = metrics(y_test, y_pred_rf_best)\n",
    "print(f\"\\nRandomForest best params: {rf_search.best_params_}\")\n",
    "print(f\"RandomForest CV best RMSE (moyenne folds): {np.sqrt(-rf_search.best_score_):.2f}\")\n",
    "tuned_results.append((\"RandomForest (tuned)\", rmse, mae, r2))\n",
    "print_scores(\"RandomForest (tuned)\", y_test, y_pred_rf_best)\n",
    "\n",
    "# Paramètres XGBoost (simples)\n",
    "if HAS_XGB:\n",
    "    param_dist_xgb = {\n",
    "        \"n_estimators\": [300, 600, 900, 1200],\n",
    "        \"max_depth\": [3, 4, 5, 6, 8],\n",
    "        \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "        \"subsample\": [0.7, 0.9, 1.0],\n",
    "        \"colsample_bytree\": [0.7, 0.9, 1.0],\n",
    "        \"min_child_weight\": [1, 3, 5],\n",
    "        # \"reg_lambda\": [1.0, 1.5, 2.0],  # tu peux décommenter pour tester la régularisation\n",
    "    }\n",
    "    xgb = XGBRegressor(objective=\"reg:squarederror\", random_state=RANDOM_STATE)\n",
    "    xgb_search = RandomizedSearchCV(\n",
    "        xgb,\n",
    "        param_distributions=param_dist_xgb,\n",
    "        n_iter=20,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        cv=5,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        refit=True,\n",
    "    )\n",
    "    xgb_search.fit(X_train, y_train)\n",
    "\n",
    "    xgb_best = xgb_search.best_estimator_\n",
    "    y_pred_xgb_best = xgb_best.predict(X_test)\n",
    "    print(f\"\\nXGBoost best params: {xgb_search.best_params_}\")\n",
    "    print(f\"XGBoost CV best RMSE (moyenne folds): {np.sqrt(-xgb_search.best_score_):.2f}\")\n",
    "    tuned_results.append((\"XGBoost (tuned)\", *metrics(y_test, y_pred_xgb_best)))\n",
    "    print_scores(\"XGBoost (tuned)\", y_test, y_pred_xgb_best)\n",
    "\n",
    "print(\"\\n=== Résumé avant vs après (sur test) ===\")\n",
    "def to_df(rows):\n",
    "    return pd.DataFrame(\n",
    "        rows, columns=[\"Model\", \"RMSE\", \"MAE\", \"R2\"]\n",
    "    ).sort_values(\"RMSE\").reset_index(drop=True)\n",
    "\n",
    "print(\"Avant tuning:\")\n",
    "print(to_df(default_results))\n",
    "print(\"\\nAprès tuning:\")\n",
    "print(to_df(tuned_results))"
   ],
   "id": "9d0e395c89da9c5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Scores par défaut (avant tuning) ===\n",
      "Baseline-Mean                  -> RMSE:12465.61  MAE: 9593.34  R²:-0.0009\n",
      "RandomForest (default)         -> RMSE: 4576.30  MAE: 2550.08  R²: 0.8651\n",
      "XGBoost non installé -> on saute cette partie (pip install xgboost)\n",
      "\n",
      "=== Tuning (RandomizedSearchCV, CV=5) ===\n",
      "\n",
      "RandomForest best params: {'n_estimators': 1200, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None, 'max_depth': 5}\n",
      "RandomForest CV best RMSE (moyenne folds): 4673.68\n",
      "RandomForest (tuned)           -> RMSE: 4379.02  MAE: 2518.53  R²: 0.8765\n",
      "\n",
      "=== Résumé avant vs après (sur test) ===\n",
      "Avant tuning:\n",
      "                    Model          RMSE          MAE        R2\n",
      "0  RandomForest (default)   4576.299916  2550.078471  0.865103\n",
      "1           Baseline-Mean  12465.610442  9593.338461 -0.000919\n",
      "\n",
      "Après tuning:\n",
      "                  Model        RMSE          MAE        R2\n",
      "0  RandomForest (tuned)  4379.02331  2518.534658  0.876483\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
